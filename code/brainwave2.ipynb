{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words('english'))\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "lines=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_lexicon():\n",
    "    lexicon=[]\n",
    "    with open('pos.txt','r') as f:\n",
    "        contents = f.readlines()\n",
    "        #all_words=word_tokenize(contents)\n",
    "        for l in contents[:lines]:\n",
    "            all_words=word_tokenize(l)\n",
    "            lexicon+=list(all_words)\n",
    "\n",
    "                \n",
    "                    \n",
    "            \n",
    "    with open('neg.txt','r') as f:\n",
    "        contents = f.readlines()\n",
    "        for l in contents[:lines]:\n",
    "            all_words=word_tokenize(l)\n",
    "            lexicon += list(all_words)\n",
    "    \n",
    "    lexicon = [lemm.lemmatize(i,pos='n') for i in lexicon]\n",
    "    w_counts = Counter(lexicon)\n",
    "    l2 = []\n",
    "    for w in w_counts:\n",
    "        #print(w_counts[w])\n",
    "        if 1000 > w_counts[w] > 50:\n",
    "            l2.append(w)\n",
    "    return l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(sample,lexicon,char):\n",
    "    import numpy as np\n",
    "    featureset=[]\n",
    "    with open(sample,'r')as f:\n",
    "        contents=f.readlines()\n",
    "        \n",
    "        for l in contents[:lines]:\n",
    "            sam_words=word_tokenize(l)\n",
    "            sam_words=[lemm.lemmatize(i) for i in sam_words]\n",
    "            features=np.zeros(len(lexicon))\n",
    "            for word in sam_words:\n",
    "                if word.lower() in lexicon:\n",
    "                    index_value=lexicon.index(word.lower())\n",
    "                    features[index_value]+=1\n",
    "            features=list(features)\n",
    "            featureset.append([features,char])\n",
    "    return featureset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(r\"C:\\Users\\PIYUSH\\Desktop\\news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2873"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[\"headline\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "news=data[\"text\"]\n",
    "headline=data[\"headline\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prepare(headline):\n",
    "    lexicon=[]\n",
    "    for i in headline:\n",
    "        all_words=word_tokenize(i)\n",
    "        lexicon+=list(all_words)\n",
    "    head_lexicon=[]\n",
    "    stop = set(stopwords.words('english'))\n",
    "    head_lexicon+=[i for i in lexicon if i not in stop]\n",
    "    head_lexicon = [lemm.lemmatize(i,pos='n') for i in head_lexicon]\n",
    "    head_words=[word.lower() for word in head_lexicon if word.isalpha()]\n",
    "    return head_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_words=data_prepare(headline)\n",
    "head_lexicons=set(head_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexcons=[]\n",
    "for i in news:\n",
    "    all_worda=word_tokenize(i)\n",
    "    lexicon+=list(all_words)\n",
    "text_lexicon=[]\n",
    "stop = set(stopwords.words('english'))\n",
    "text_lexicon+=[i for i in lexicon if i not in stop]\n",
    "text_lexicon = [lemm.lemmatize(i,pos='n') for i in text_lexicon]\n",
    "text_words=[word.lower() for word in text_lexicon if word.isalpha()]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5396"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(text_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=\"So we propose a solution which uses AI at its core to save the farmer some time and provide him with flexibility to either get recommendation regarding the suitable crop given the soil state and also to get advice on soil condition and how to improve it for a particular crop plantation. We intend to introduce a device, i.e a microscope fitted with camera and an embedded system that will help to classify and detects dangerous bacteria and nutrient content of the soil. This device will require sample of soil and an input specifying a crop the farmer wishes to grow on his field or he can leave it blank if he wants recommendation. This system will run continuously in real time. The cities can install this device across different farms and they will be able to monitor soil quality as well as contamination continuously.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "825\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "for i in s:\n",
    "    c+=1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
